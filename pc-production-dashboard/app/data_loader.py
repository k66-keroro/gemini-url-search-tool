"""
PCè£½é€ å°‚ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼

éå»ãƒ‡ãƒ¼ã‚¿ï¼ˆæœˆæ¬¡ZM29ï¼‰ã¨å½“æ—¥ãƒ‡ãƒ¼ã‚¿ï¼ˆMESï¼‰ã®çµ±åˆå‡¦ç†
"""

import pandas as pd
import sqlite3
from pathlib import Path
import datetime
import logging
import chardet
import shutil
import os

class PCProductionDataLoader:
    def __init__(self, db_path=None):
        # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’è¨­å®š
        if db_path is None:
            base_dir = Path(__file__).parent.parent
            self.db_path = base_dir / "data" / "sqlite" / "pc_production.db"
        else:
            self.db_path = Path(db_path)
        
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ã‚µãƒ¼ãƒãƒ¼ãƒ‘ã‚¹è¨­å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§.csvã‹ã‚‰å–å¾—ï¼‰
        self.server_paths = {
            'KANSEI_JISSEKI': r'\\fssha01\common\HOST\MES\KANSEI_JISSEKI.txt',
            'KOUSU_JISSEKI': r'\\fssha01\common\HOST\MES\KOUSU_JISSEKI.txt',
            'KOUTEI_JISSEKI': r'\\fssha01\common\HOST\MES\KOUTEI_JISSEKI.txt',
            'SASIZU_JISSEKI': r'\\fssha01\common\HOST\MES\SASIZU_JISSEKI.txt',
            'ZP51N': r'\\fssha01\common\HOST\ZP51N\ZP51N.TXT',
            'PC_SEISAN_KANRI': r'\\fsshi01\é›»æºæ©Ÿå™¨è£½é€ æœ¬éƒ¨å…±æœ‰\39ã€€éƒ¨æç®¡ç†èª²â‡’ä»–éƒ¨ç½²å…±æœ‰\01     PCå·¥ç¨‹é€²æ—ç®¡ç†\2020.5æœˆï½PCç”Ÿç”£_ç”Ÿç”£ç®¡ç†_20250724.xlsx'
        }
    
    def detect_encoding(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º"""
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # æœ€åˆã®10KBã‚’èª­ã¿å–ã‚Š
                result = chardet.detect(raw_data)
                encoding = result['encoding']
                confidence = result['confidence']
                
                self.logger.info(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º: {file_path.name} -> {encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
                
                # CP932ã®å ´åˆã¯shift_jisã¨ã—ã¦æ‰±ã†
                if encoding and 'cp932' in encoding.lower():
                    encoding = 'shift_jis'
                
                return encoding if confidence > 0.7 else 'utf-8'
        except Exception as e:
            self.logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºå¤±æ•—: {file_path} -> ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆutf-8ä½¿ç”¨")
            return 'utf-8'
    
    def load_historical_zm29_data(self):
        """éå»ã®ZM29æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’è¨­å®š
        base_dir = Path(__file__).parent.parent
        historical_path = base_dir / "data" / "zm29_Monthly_performance"
        
        # claude-testã‹ã‚‰ã®ãƒ‘ã‚¹ã‚‚è©¦ã™
        if not historical_path.exists():
            claude_test_path = Path("../data/zm29_Monthly_performance")
            if claude_test_path.exists():
                historical_path = claude_test_path
        
        if not historical_path.exists():
            self.logger.error(f"éå»ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {historical_path}")
            return pd.DataFrame()
        
        all_data = []
        
        # æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †ç•ªã«èª­ã¿è¾¼ã¿
        for file_path in sorted(historical_path.glob("ZM29_*.txt")):
            try:
                self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path.name}")
                
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
                encoding = self.detect_encoding(file_path)
                
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                df = pd.read_csv(
                    file_path,
                    delimiter='\t',
                    encoding=encoding,
                    dtype=str,
                    on_bad_lines='skip'
                )
                
                # å¹´æœˆæƒ…å ±ã‚’è¿½åŠ 
                year_month = file_path.stem.split('_')[1]  # ZM29_202404 -> 202404
                df['ãƒ‡ãƒ¼ã‚¿å¹´æœˆ'] = year_month
                df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = 'éå»ãƒ‡ãƒ¼ã‚¿'
                
                all_data.append(df)
                self.logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {file_path.name} ({len(df)}è¡Œ)")
                
            except Exception as e:
                self.logger.error(f"éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path.name} - {e}")
                continue
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: ç·è¡Œæ•° {len(combined_df)}")
            return combined_df
        else:
            return pd.DataFrame()
    
    def copy_current_data_from_server(self):
        """ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼"""
        base_dir = Path(__file__).parent.parent
        current_data_path = base_dir / "data" / "current"
        current_data_path.mkdir(parents=True, exist_ok=True)
        
        copied_files = []
        
        for data_name, server_path in self.server_paths.items():
            try:
                server_file = Path(server_path)
                local_file = current_data_path / server_file.name
                
                if server_file.exists():
                    shutil.copy2(server_file, local_file)
                    copied_files.append(local_file)
                    self.logger.info(f"ã‚µãƒ¼ãƒãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼å®Œäº†: {server_file.name}")
                else:
                    self.logger.warning(f"ã‚µãƒ¼ãƒãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {server_path}")
                    
            except Exception as e:
                self.logger.error(f"ã‚µãƒ¼ãƒãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼: {data_name} - {e}")
                continue
        
        return copied_files
    
    def load_current_zm29_data(self):
        """å½“æ—¥ã®ZM29ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆKANSEI_JISSEKIã‹ã‚‰ç”Ÿæˆï¼‰"""
        base_dir = Path(__file__).parent.parent
        current_file = base_dir / "data" / "current" / "KANSEI_JISSEKI.txt"
        
        if not current_file.exists():
            self.logger.warning("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return pd.DataFrame()
        
        try:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
            encoding = self.detect_encoding(current_file)
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = pd.read_csv(
                current_file,
                delimiter='\t',
                encoding=encoding,
                dtype=str,
                on_bad_lines='skip'
            )
            
            # KANSEI_JISSEKIã®ã‚«ãƒ©ãƒ ã‚’ZM29å½¢å¼ã«ãƒãƒƒãƒ”ãƒ³ã‚°
            column_mapping = {
                'è»¢è¨˜æ—¥ä»˜': 'è»¢è¨˜æ—¥ä»˜',
                'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æŒ‡å›³ç•ªå·': 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æŒ‡å›³ç•ªå·',
                'å“ç›®ã‚³ãƒ¼ãƒ‰': 'å“ç›®ã‚³ãƒ¼ãƒ‰',
                'å“ç›®ãƒ†ã‚­ã‚¹ãƒˆ': 'å“ç›®ãƒ†ã‚­ã‚¹ãƒˆ',
                'å®Œæˆæ•°': 'å®Œæˆæ•°',
                'å˜ä¾¡': 'å˜ä¾¡',
                'MRPç®¡ç†è€…': 'MRPç®¡ç†è€…'
            }
            
            # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡ºã—ã¦ãƒªãƒãƒ¼ãƒ 
            available_columns = [col for col in column_mapping.keys() if col in df.columns]
            if not available_columns:
                self.logger.warning("KANSEI_JISSEKIã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                self.logger.info(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(df.columns)}")
                # æœ€ä½é™ã®ã‚«ãƒ©ãƒ ã‚’ä½œæˆ
                df = df.copy()
                if 'è»¢è¨˜æ—¥ä»˜' not in df.columns and len(df.columns) > 0:
                    df['è»¢è¨˜æ—¥ä»˜'] = datetime.datetime.now().strftime('%Y%m%d')
                if 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æŒ‡å›³ç•ªå·' not in df.columns and len(df.columns) > 0:
                    df['ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æŒ‡å›³ç•ªå·'] = df.iloc[:, 0] if len(df.columns) > 0 else 'UNKNOWN'
                if 'å“ç›®ã‚³ãƒ¼ãƒ‰' not in df.columns and len(df.columns) > 1:
                    df['å“ç›®ã‚³ãƒ¼ãƒ‰'] = df.iloc[:, 1] if len(df.columns) > 1 else 'UNKNOWN'
                if 'å®Œæˆæ•°' not in df.columns:
                    df['å®Œæˆæ•°'] = 1
                if 'å˜ä¾¡' not in df.columns:
                    df['å˜ä¾¡'] = 0
                if 'MRPç®¡ç†è€…' not in df.columns:
                    df['MRPç®¡ç†è€…'] = 'PC1'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            else:
                df = df[available_columns].copy()
            
            # å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’è¿½åŠ 
            today = datetime.datetime.now().strftime('%Y%m%d')
            df['ãƒ‡ãƒ¼ã‚¿å¹´æœˆ'] = today[:6]  # YYYYMM
            df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = 'å½“æ—¥ãƒ‡ãƒ¼ã‚¿'
            df['æ›´æ–°æ™‚åˆ»'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            self.logger.info(f"å½“æ—¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
            return df
            
        except Exception as e:
            self.logger.error(f"å½“æ—¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def calculate_monthly_week(self, date_str):
        """æœˆåˆ¥é€±åŒºåˆ†ã‚’è¨ˆç®—ï¼ˆ1-5é€±ï¼‰"""
        try:
            if pd.isna(date_str) or date_str == '':
                return None
                
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
            if len(str(date_str)) == 8:  # YYYYMMDD
                date = pd.to_datetime(str(date_str), format='%Y%m%d')
            else:
                date = pd.to_datetime(date_str)
            
            # æœˆã®æœ€åˆã®æ—¥
            first_day = date.replace(day=1)
            
            # æœˆã®æœ€åˆã®æœˆæ›œæ—¥ã‚’è¦‹ã¤ã‘ã‚‹
            days_to_monday = (7 - first_day.weekday()) % 7
            first_monday = first_day + pd.Timedelta(days=days_to_monday)
            
            # å¯¾è±¡æ—¥ãŒæœ€åˆã®æœˆæ›œæ—¥ã‚ˆã‚Šå‰ã®å ´åˆã¯ç¬¬1é€±
            if date < first_monday:
                return 1
            
            # æœ€åˆã®æœˆæ›œæ—¥ã‹ã‚‰ã®çµŒéæ—¥æ•°ã§é€±ã‚’è¨ˆç®—
            days_from_first_monday = (date - first_monday).days
            week_number = min(days_from_first_monday // 7 + 2, 5)
            
            return week_number
            
        except Exception as e:
            self.logger.warning(f"é€±åŒºåˆ†è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {date_str} - {e}")
            return None
    
    def process_zm29_data(self, df):
        """ZM29ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        if df.empty:
            return df
        
        processed_df = df.copy()
        
        # æ—¥ä»˜åˆ—ã®å‡¦ç†
        if 'è»¢è¨˜æ—¥ä»˜' in processed_df.columns:
            processed_df['è»¢è¨˜æ—¥ä»˜'] = pd.to_datetime(processed_df['è»¢è¨˜æ—¥ä»˜'], errors='coerce')
            
            # æœˆåˆ¥é€±åŒºåˆ†ã‚’è¨ˆç®—
            processed_df['æœˆåˆ¥é€±åŒºåˆ†'] = processed_df['è»¢è¨˜æ—¥ä»˜'].apply(
                lambda x: self.calculate_monthly_week(x) if pd.notna(x) else None
            )
        
        # æ•°å€¤åˆ—ã®å‡¦ç†
        numeric_columns = ['å®Œæˆæ•°', 'è¨ˆç”»æ•°', 'å˜ä¾¡']
        for col in numeric_columns:
            if col in processed_df.columns:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0)
        
        # é‡‘é¡è¨ˆç®—
        if 'å®Œæˆæ•°' in processed_df.columns and 'å˜ä¾¡' in processed_df.columns:
            processed_df['é‡‘é¡'] = processed_df['å®Œæˆæ•°'] * processed_df['å˜ä¾¡']
        
        # PCè£½é€ ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if 'MRPç®¡ç†è€…' in processed_df.columns:
            processed_df = processed_df[processed_df['MRPç®¡ç†è€…'].str.contains('PC', na=False)]
        
        return processed_df
    
    def save_to_database(self, df, table_name):
        """ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç½®ãæ›ãˆã¦ä¿å­˜
            df.to_sql(table_name, conn, if_exists='replace', index=False)
            
            conn.close()
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {table_name} ({len(df)}è¡Œ)")
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {table_name} - {e}")
    
    def integrate_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆå‡¦ç†"""
        self.logger.info("PCè£½é€ ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†é–‹å§‹")
        
        # 1. ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
        self.logger.info("Step 1: ã‚µãƒ¼ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼")
        self.copy_current_data_from_server()
        
        # 2. éå»ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        self.logger.info("Step 2: éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        historical_data = self.load_historical_zm29_data()
        
        # 3. å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        self.logger.info("Step 3: å½“æ—¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        current_data = self.load_current_zm29_data()
        
        # 4. ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
        self.logger.info("Step 4: ãƒ‡ãƒ¼ã‚¿çµ±åˆ")
        all_zm29_data = []
        
        try:
            if not historical_data.empty:
                processed_historical = self.process_zm29_data(historical_data)
                if not processed_historical.empty:
                    all_zm29_data.append(processed_historical)
                    self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(processed_historical)}è¡Œ")
            
            if not current_data.empty:
                processed_current = self.process_zm29_data(current_data)
                if not processed_current.empty:
                    all_zm29_data.append(processed_current)
                    self.logger.info(f"å½“æ—¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(processed_current)}è¡Œ")
            
            if all_zm29_data:
                # ã‚«ãƒ©ãƒ ã®çµ±ä¸€
                common_columns = None
                for data in all_zm29_data:
                    if common_columns is None:
                        common_columns = set(data.columns)
                    else:
                        common_columns = common_columns.intersection(set(data.columns))
                
                if common_columns:
                    # å…±é€šã‚«ãƒ©ãƒ ã®ã¿ã§çµ±åˆ
                    common_columns = list(common_columns)
                    unified_data = []
                    for data in all_zm29_data:
                        unified_data.append(data[common_columns])
                    
                    integrated_data = pd.concat(unified_data, ignore_index=True)
                    
                    # é‡è¤‡é™¤å»ï¼ˆåŒã˜æ—¥ä»˜ãƒ»å“ç›®ãƒ»æŒ‡å›³ç•ªå·ã®å ´åˆã¯å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆï¼‰
                    if 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹' in integrated_data.columns:
                        integrated_data = integrated_data.sort_values('ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', ascending=False)  # å½“æ—¥ãƒ‡ãƒ¼ã‚¿ãŒå…ˆ
                        
                        # é‡è¤‡é™¤å»ã®ã‚­ãƒ¼ã‚’å‹•çš„ã«æ±ºå®š
                        dedup_keys = []
                        for key in ['è»¢è¨˜æ—¥ä»˜', 'å“ç›®ã‚³ãƒ¼ãƒ‰', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æŒ‡å›³ç•ªå·']:
                            if key in integrated_data.columns:
                                dedup_keys.append(key)
                        
                        if dedup_keys:
                            integrated_data = integrated_data.drop_duplicates(
                                subset=dedup_keys,
                                keep='first'
                            )
                    
                    # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    self.logger.info("Step 5: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜")
                    self.save_to_database(integrated_data, 'pc_production_zm29')
                    
                    self.logger.info(f"PCè£½é€ ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: ç·è¡Œæ•° {len(integrated_data)}")
                    return integrated_data
                else:
                    self.logger.error("çµ±åˆå¯èƒ½ãªå…±é€šã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
                    return pd.DataFrame()
            else:
                self.logger.warning("çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
            if not historical_data.empty:
                self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {list(historical_data.columns)}")
            if not current_data.empty:
                self.logger.info(f"å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {list(current_data.columns)}")
            return pd.DataFrame()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    loader = PCProductionDataLoader()
    result = loader.integrate_all_data()
    
    if not result.empty:
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(result)}è¡Œ")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {result['è»¢è¨˜æ—¥ä»˜'].min()} ï½ {result['è»¢è¨˜æ—¥ä»˜'].max()}")
        print(f"ğŸ­ MRPç®¡ç†è€…: {result['MRPç®¡ç†è€…'].unique()}")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()