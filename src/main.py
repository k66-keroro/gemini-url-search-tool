import sys
from pathlib import Path
import logging
import pandas as pd
from typing import Dict, List
import sqlite3
import json
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
PROJECT_ROOT = str(Path(__file__).resolve().parents[1])
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from config.constants import Paths
from src.core.sqlite_manager import SQLiteManager
# ç‰¹æ®Šå‡¦ç†ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.processors.zp138_processor import ZP138Processor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Paths().LOGS / "main.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def get_index_definitions(file_path: Path) -> Dict[str, List[str]]:
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    index_info = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            next(f) # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
            for line in f:
                parts = [p.strip() for p in line.strip().split('\t')]
                if len(parts) >= 2 and parts[0]:
                    table_name = parts[0].lower()
                    index_cols = [p for p in parts[1:3] if p]
                    if index_cols:
                        index_info[table_name] = index_cols
    except FileNotFoundError:
        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    except Exception as e:
        logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    return index_info

def _sanitize_table_name(table_name: str) -> str:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’é©åˆ‡ã«å¤‰æ›ï¼ˆæ—¥æœ¬èªã‚„ç‰¹æ®Šæ–‡å­—ã‚’é¿ã‘ã‚‹ï¼‰
    
    Args:
        table_name: å…ƒã®ãƒ†ãƒ¼ãƒ–ãƒ«å
        
    Returns:
        å¤‰æ›å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    import re
    
    # æ—¥æœ¬èªæ–‡å­—ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯
    has_japanese = re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', table_name)
    
    # æ—¥æœ¬èªæ–‡å­—ã‚’å«ã‚€å ´åˆã¯ã€ãƒ­ãƒ¼ãƒå­—ã«å¤‰æ›ã™ã‚‹ã‹ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ã‚‹
    if has_japanese:
        # ç°¡æ˜“çš„ãªå¤‰æ›: æ—¥æœ¬èªãƒ†ãƒ¼ãƒ–ãƒ«åã«ã¯t_ã‚’ä»˜ã‘ã‚‹
        sanitized = f"t_{hash(table_name) % 10000:04d}"
    else:
        # è‹±æ•°å­—ä»¥å¤–ã®æ–‡å­—ã‚’_ã«ç½®æ›
        sanitized = re.sub(r'[^a-zA-Z0-9]', '_', table_name)
        
        # é€£ç¶šã™ã‚‹_ã‚’å˜ä¸€ã®_ã«ç½®æ›
        sanitized = re.sub(r'_+', '_', sanitized)
        
        # å…ˆé ­ãŒæ•°å­—ã®å ´åˆã€t_ã‚’ä»˜ã‘ã‚‹
        if sanitized[0].isdigit():
            sanitized = f"t_{sanitized}"
            
        # å…ˆé ­ã¨æœ«å°¾ã®_ã‚’å‰Šé™¤
        sanitized = sanitized.strip('_')
        
    return sanitized

def run_priority_batch_import():
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ã§SQLiteã«æ ¼ç´ã—ã€å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’æœ€çµ‚åŒ–ï¼ˆä¸»ã‚­ãƒ¼è¿½åŠ ï¼‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼‰ã—ã¾ã™ã€‚
    """
    try:
        paths = Paths()
        priority_csv_path = paths.PROJECT_ROOT / 'config' / 'ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§.csv'
        index_def_path = paths.PROJECT_ROOT / 'config' / 'index_definitions.txt'
        db_path = paths.SQLITE_DB
        raw_dir = paths.RAW_DATA
    except Exception as e:
        logger.error(f"âŒ è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False

    if not priority_csv_path.exists() or not raw_dir.exists():
        logger.error(f"âŒ å„ªå…ˆé †ä½ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return False

    logger.info(f"â–¶ï¸ ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ - DB: {db_path}")

    manager = SQLiteManager(db_path)
    index_definitions = get_index_definitions(index_def_path)
    if index_definitions:
        logger.info(f"ğŸ”‘ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(index_definitions)}ä»¶")

    try:
        df_priority = pd.read_csv(priority_csv_path)
        df_priority['é‡è¦åº¦'] = df_priority['é‡è¦åº¦'].fillna('')
        priority_map = {'zz': 0, 'z': 1}
        df_priority['sort_key'] = df_priority['é‡è¦åº¦'].map(priority_map).fillna(99)
        df_priority = df_priority.sort_values(by='sort_key').reset_index(drop=True)
        logger.info(f"ğŸ“Š å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(df_priority)}")
    except Exception as e:
        logger.error(f"âŒ å„ªå…ˆé †ä½CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False

    success_files, failed_files, skipped_files, structure_failed_tables = [], [], [], []
    total_files = len(df_priority)

    for i, row in df_priority.iterrows():
        file_name = row.get('ãƒ•ã‚¡ã‚¤ãƒ«å')
        table_name_orig = row.get('ãƒ†ãƒ¼ãƒ–ãƒ«å')

        if not file_name or not table_name_orig:
            logger.warning(f"âš ï¸ {i+1}è¡Œç›®: 'ãƒ•ã‚¡ã‚¤ãƒ«å'ã¾ãŸã¯'ãƒ†ãƒ¼ãƒ–ãƒ«å'ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’é©åˆ‡ã«å¤‰æ›ï¼ˆæ—¥æœ¬èªã‚„ç‰¹æ®Šæ–‡å­—ã‚’é¿ã‘ã‚‹ï¼‰
        table_name = _sanitize_table_name(str(table_name_orig).lower())
        file_path = raw_dir / str(file_name)
        logger.info(f"--- å‡¦ç†ä¸­ ({i+1}/{total_files}): {file_name} -> {table_name} ---")
        
        # å…ƒã®ãƒ†ãƒ¼ãƒ–ãƒ«åã¨å¤‰æ›å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«åãŒç•°ãªã‚‹å ´åˆã¯ãƒ­ã‚°ã«è¨˜éŒ²
        if table_name != str(table_name_orig).lower():
            logger.info(f"ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å¤‰æ›ã—ã¾ã—ãŸ: {table_name_orig} -> {table_name}")

        if not file_path.exists():
            logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {file_path}")
            skipped_files.append(file_name)
            continue

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†è¨­å®šã‚’å–å¾—
        config = manager.get_file_processing_config(file_name)
        index_cols = index_definitions.get(table_name, [])
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºç«‹
        with sqlite3.connect(db_path) as conn:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            success, error_message = manager.bulk_insert_from_file(
                conn=conn,
                file_path=file_path,
                table_name=table_name,
                encoding=config['encoding'],
                quoting=config['quoting']
            )
            
            if not success:
                failed_files.append(file_name)
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {file_name} - {error_message}")
                continue
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’æœ€çµ‚åŒ–ï¼ˆä¸»ã‚­ãƒ¼è¿½åŠ ï¼‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼‰
            success2, error_message2 = manager.finalize_table_structure(
                conn=conn,
                table_name=table_name,
                primary_key_columns=None,  # _rowid_ã‚’ä½¿ç”¨
                index_columns=index_cols
            )
            
            if not success2:
                structure_failed_tables.append(table_name)
                logger.error(f"âŒ æ§‹é€ æœ€çµ‚åŒ–å¤±æ•—: {table_name} - {error_message2}")
                continue
        
        success_files.append(file_name)
        logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸ: {file_name}")

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    logger.info("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    summary_report = f"""
=== å‡¦ç†ã‚µãƒãƒªãƒ¼ ===
  ğŸ“Š å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ç·æ•°: {total_files}
  âœ… æˆåŠŸ: {len(success_files)}
  âŒ å¤±æ•—: {len(failed_files)}
  â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {len(skipped_files)}
  âš ï¸ æ§‹é€ æœ€çµ‚åŒ–å¤±æ•—: {len(structure_failed_tables)}
"""
    print(summary_report)
    
    if failed_files:
        print("\n--- å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ« ---")
        for f in failed_files: print(f"  âŒ {f}")
            
    if skipped_files:
        print("\n--- ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« ---")
        for f in skipped_files: print(f"  â­ï¸ {f}")
        
    if structure_failed_tables:
        print("\n--- æ§‹é€ æœ€çµ‚åŒ–ã«å¤±æ•—ã—ãŸãƒ†ãƒ¼ãƒ–ãƒ« ---")
        for t in structure_failed_tables: print(f"  âš ï¸ {t}")

    return len(failed_files) == 0 and len(structure_failed_tables) == 0


def run_special_processors(config_path=None):
    """
    ç‰¹æ®Šå‡¦ç†ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        bool: ã™ã¹ã¦ã®å‡¦ç†ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€ãã‚Œä»¥å¤–ã¯False
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    config = {
        "processors": {
            "zp138": {
                "enabled": True,
                "config_file": "config/processors/zp138_config.json"
            }
        }
    }
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯èª­ã¿è¾¼ã‚€
    if config_path and os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                file_config = json.load(f)
                # è¨­å®šã‚’ãƒãƒ¼ã‚¸
                if "processors" in file_config:
                    config["processors"].update(file_config["processors"])
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    success = True
    
    # ZP138ãƒ—ãƒ­ã‚»ãƒƒã‚µã®å®Ÿè¡Œ
    if config["processors"].get("zp138", {}).get("enabled", False):
        logger.info("ğŸ”„ ZP138ç‰¹æ®Šå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        
        # ZP138ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        zp138_config = {}
        config_file = config["processors"]["zp138"].get("config_file")
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    zp138_config = json.load(f)
            except Exception as e:
                logger.error(f"ZP138è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # ZP138ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’å®Ÿè¡Œ
        processor = ZP138Processor(zp138_config)
        if not processor.process():
            logger.error("âŒ ZP138å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            success = False
        else:
            logger.info("âœ… ZP138å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    
    return success

if __name__ == "__main__":
    try:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
        import argparse
        parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿æ›´æ–°å‡¦ç†ãƒãƒƒãƒ')
        parser.add_argument('--special-only', action='store_true', help='ç‰¹æ®Šå‡¦ç†ã®ã¿ã‚’å®Ÿè¡Œ')
        parser.add_argument('--config', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
        args = parser.parse_args()
        
        is_successful = True
        
        # æ¨™æº–ãƒãƒƒãƒå‡¦ç†
        if not args.special_only:
            logger.info("ğŸ”„ æ¨™æº–ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
            is_successful = run_priority_batch_import()
            
        # ç‰¹æ®Šå‡¦ç†
        logger.info("ğŸ”„ ç‰¹æ®Šå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        special_success = run_special_processors(args.config)
        is_successful = is_successful and special_success
        
        exit_code = 0 if is_successful else 1
        logger.info(f"ğŸ ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code})")
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        sys.exit(130)
    except Exception as e:
        logger.exception(f"ğŸ’¥ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)