#!/usr/bin/env python3
"""
Real Content Search - å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„URLã‚’å–å¾—ã™ã‚‹æ¤œç´¢ãƒ„ãƒ¼ãƒ«
"""

import streamlit as st
import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import google.generativeai as genai
import time
from urllib.parse import quote_plus, urlparse, urljoin
import re

# Load environment variables
load_dotenv()

def search_with_real_urls(query, num_results=8):
    """å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„URLã‚’å–å¾—ã™ã‚‹æ¤œç´¢"""
    results = []
    
    # 1. Wikipediaæ¤œç´¢ï¼ˆå®Ÿéš›ã®è¨˜äº‹URLï¼‰
    try:
        wiki_url = f"https://en.wikipedia.org/w/api.php?action=opensearch&search={quote_plus(query)}&limit=3&format=json"
        response = requests.get(wiki_url, timeout=10)
        data = response.json()
        
        if len(data) >= 4:
            titles = data[1]
            descriptions = data[2]
            urls = data[3]
            
            for i in range(min(len(titles), 3)):
                results.append({
                    'title': f"{titles[i]} - Wikipedia",
                    'url': urls[i],
                    'description': descriptions[i] if i < len(descriptions) else "Wikipedia article",
                    'source': 'Wikipedia',
                    'confidence': 0.9
                })
    except:
        pass
    
    # 2. GitHubæ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒªãƒã‚¸ãƒˆãƒªURLï¼‰
    try:
        tech_keywords = ['python', 'javascript', 'react', 'vue', 'angular', 'node', 'docker', 'kubernetes', 'aws', 'api', 'framework', 'library', 'vba', 'visual basic', 'excel', 'code', 'programming']
        if any(keyword in query.lower() for keyword in tech_keywords):
            github_url = f"https://api.github.com/search/repositories?q={quote_plus(query)}&sort=stars&order=desc&per_page=2"
            response = requests.get(github_url, timeout=10)
            data = response.json()
            
            if 'items' in data:
                for repo in data['items'][:2]:
                    results.append({
                        'title': f"{repo['name']} - GitHub",
                        'url': repo['html_url'],
                        'description': repo.get('description', 'GitHub repository'),
                        'source': 'GitHub',
                        'confidence': 0.8
                    })
    except:
        pass
    
    # 3. DuckDuckGo Instant Answerï¼ˆå®Ÿéš›ã®URLï¼‰
    try:
        ddg_url = f"https://api.duckduckgo.com/?q={quote_plus(query)}&format=json&no_html=1&skip_disambig=1"
        response = requests.get(ddg_url, timeout=10)
        data = response.json()
        
        # Related Topics ã‹ã‚‰å®Ÿéš›ã®URL
        if 'RelatedTopics' in data:
            for topic in data['RelatedTopics'][:3]:
                if isinstance(topic, dict) and 'FirstURL' in topic and topic.get('FirstURL'):
                    results.append({
                        'title': topic.get('Text', 'DuckDuckGo Result')[:80] + '...',
                        'url': topic.get('FirstURL'),
                        'description': topic.get('Text', 'Related information'),
                        'source': 'DuckDuckGo',
                        'confidence': 0.7
                    })
        
        # Abstract URL
        if data.get('AbstractURL') and data.get('Abstract'):
            results.append({
                'title': data.get('Heading', 'Main Result'),
                'url': data.get('AbstractURL'),
                'description': data.get('Abstract', 'Main information'),
                'source': 'DuckDuckGo',
                'confidence': 0.8
            })
    except:
        pass
    
    # 4. æ‰‹å‹•ã§å³é¸ã—ãŸå®Ÿéš›ã®URL
    manual_sites = {
        'python': [
            {'title': 'Python.org - Official Site', 'url': 'https://www.python.org/', 'description': 'Official Python programming language website', 'confidence': 1.0},
            {'title': 'Python Tutorial', 'url': 'https://docs.python.org/3/tutorial/', 'description': 'Official Python tutorial', 'confidence': 1.0},
            {'title': 'Real Python', 'url': 'https://realpython.com/', 'description': 'High-quality Python tutorials', 'confidence': 0.9},
        ],
        'vba': [
            {'title': 'Microsoft VBA Documentation', 'url': 'https://docs.microsoft.com/en-us/office/vba/', 'description': 'Official Microsoft VBA documentation', 'confidence': 1.0},
            {'title': 'Excel VBA Tutorial', 'url': 'https://www.excel-easy.com/vba.html', 'description': 'Excel VBA tutorial and examples', 'confidence': 0.9},
        ],
        'javascript': [
            {'title': 'MDN JavaScript Guide', 'url': 'https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide', 'description': 'Comprehensive JavaScript guide', 'confidence': 1.0},
            {'title': 'JavaScript.info', 'url': 'https://javascript.info/', 'description': 'Modern JavaScript tutorial', 'confidence': 0.9},
        ],
        'react': [
            {'title': 'React Official Tutorial', 'url': 'https://react.dev/learn', 'description': 'Official React learning guide', 'confidence': 1.0},
        ],
        'æ–™ç†': [
            {'title': 'ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³ ãƒ¬ã‚·ãƒ”', 'url': 'https://www.kikkoman.co.jp/homecook/search/', 'description': 'ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³ã®æ–™ç†ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆ', 'confidence': 0.9},
            {'title': 'ã‚¯ãƒ©ã‚·ãƒ«', 'url': 'https://www.kurashiru.com/', 'description': 'å‹•ç”»ã§å­¦ã¹ã‚‹æ–™ç†ãƒ¬ã‚·ãƒ”', 'confidence': 0.8},
            {'title': 'Delish Kitchen', 'url': 'https://delishkitchen.tv/', 'description': 'å‹•ç”»ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆ', 'confidence': 0.8},
        ],
        'ãƒ¬ã‚·ãƒ”': [
            {'title': 'ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³ ãƒ¬ã‚·ãƒ”', 'url': 'https://www.kikkoman.co.jp/homecook/search/', 'description': 'ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³ã®æ–™ç†ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆ', 'confidence': 0.9},
            {'title': 'ã‚¯ãƒ©ã‚·ãƒ«', 'url': 'https://www.kurashiru.com/', 'description': 'å‹•ç”»ã§å­¦ã¹ã‚‹æ–™ç†ãƒ¬ã‚·ãƒ”', 'confidence': 0.8},
            {'title': 'ã¿ã‚“ãªã®ãã‚‡ã†ã®æ–™ç†', 'url': 'https://www.kyounoryouri.jp/', 'description': 'NHKã®æ–™ç†ç•ªçµ„ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆ', 'confidence': 0.8},
        ],
        'å¥åº·': [
            {'title': 'åšç”ŸåŠ´åƒçœ', 'url': 'https://www.mhlw.go.jp/', 'description': 'å…¬å¼å¥åº·æƒ…å ±', 'confidence': 1.0},
            {'title': 'ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢å¤§å­¦', 'url': 'https://www.skincare-univ.com/', 'description': 'åŒ»å¸«ç›£ä¿®ã®å¥åº·æƒ…å ±', 'confidence': 0.8},
        ],
        'æ—…è¡Œ': [
            {'title': 'ã˜ã‚ƒã‚‰ã‚“', 'url': 'https://www.jalan.net/', 'description': 'å›½å†…æ—…è¡Œäºˆç´„ã‚µã‚¤ãƒˆ', 'confidence': 0.9},
            {'title': 'æ¥½å¤©ãƒˆãƒ©ãƒ™ãƒ«', 'url': 'https://travel.rakuten.co.jp/', 'description': 'æ—…è¡Œäºˆç´„ãƒ»ãƒ›ãƒ†ãƒ«æ¤œç´¢', 'confidence': 0.9},
        ],
        'å­¦ç¿’': [
            {'title': 'Khan Academy', 'url': 'https://www.khanacademy.org/', 'description': 'ç„¡æ–™ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ', 'confidence': 0.9},
            {'title': 'Coursera', 'url': 'https://www.coursera.org/', 'description': 'å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è¬›åº§', 'confidence': 0.9},
        ],
    }
    
    query_lower = query.lower()
    for keyword, sites in manual_sites.items():
        if keyword in query_lower:
            for site in sites:
                site['source'] = 'Curated'
                results.append(site)
    
    return results[:num_results]

def scrape_search_results(query, num_results=3):
    """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰å®Ÿéš›ã®çµæœURLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå®Ÿé¨“çš„ï¼‰"""
    results = []
    
    try:
        # DuckDuckGo HTMLãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆrobots.txtã«æ³¨æ„ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        response = requests.get(search_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # DuckDuckGoã®æ¤œç´¢çµæœã‚’è§£æ
            search_results = soup.find_all('a', class_='result__a')
            
            for i, link in enumerate(search_results[:num_results]):
                href = link.get('href')
                title = link.get_text().strip()
                
                if href and title and not href.startswith('/'):
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = 'https://duckduckgo.com' + href
                    
                    results.append({
                        'title': title[:100] + '...' if len(title) > 100 else title,
                        'url': href,
                        'description': f'Search result for {query}',
                        'source': 'DuckDuckGo Scrape',
                        'confidence': 0.6
                    })
    except Exception as e:
        st.warning(f"Scraping failed: {e}")
    
    return results

def fetch_and_analyze_content(url, query, api_key):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦åˆ†æ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title = soup.find('title')
        page_title = title.get_text().strip() if title else "No Title"
        
        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        if len(text) > 8000:
            text = text[:8000] + "..."
        
        # Geminiåˆ†æ
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        analysis_prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€ãƒšãƒ¼ã‚¸æƒ…å ±ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {page_title}
URL: {url}

ã€åˆ†æè¦æ±‚ã€‘
1. è¦ç´„ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰
2. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼ˆ3-5å€‹ï¼‰
3. æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£æ€§ï¼ˆ1-5ã§è©•ä¾¡ï¼‰
4. ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹å…·ä½“çš„ãªä¾¡å€¤

ã€å†…å®¹ã€‘
{text[:6000]}

ã€åˆ†æçµæœã€‘
"""
        
        analysis_response = model.generate_content(analysis_prompt)
        
        return {
            'success': True,
            'title': page_title,
            'url': url,
            'content_length': len(text),
            'analysis': analysis_response.text
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def main():
    st.set_page_config(
        page_title="Real Content Search Tool",
        page_icon="ğŸ¯",
        layout="wide"
    )
    
    st.title("ğŸ¯ Real Content Search Tool")
    st.markdown("**å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„URLã‚’å–å¾—ã™ã‚‹æ¤œç´¢ãƒ„ãƒ¼ãƒ«**")
    st.markdown("---")
    
    # API key check
    api_key = os.getenv("GEMINI_API_KEY")
    
    if api_key:
        st.success("âœ… Gemini API key configured")
    else:
        st.error("âŒ Gemini API key not found")
        st.stop()
    
    # Search Section
    st.subheader("ğŸ” Real URL Search")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        query = st.text_input(
            "Search Query",
            placeholder="ä¾‹: Python å­¦ç¿’",
            help="æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›"
        )
    
    with col2:
        include_scraping = st.checkbox("Include Scraping", value=True, help="å®Ÿé¨“çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¤œç´¢ã‚’å«ã‚ã‚‹ï¼ˆæ¨å¥¨ï¼‰")
    
    if st.button("ğŸ¯ Search Real URLs", type="primary"):
        if query:
            with st.spinner("Searching for real content URLs..."):
                # åŸºæœ¬æ¤œç´¢
                results = search_with_real_urls(query, 6)
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¤œç´¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if include_scraping:
                    scraped_results = scrape_search_results(query, 3)
                    results.extend(scraped_results)
                
                if results:
                    st.session_state.search_results = results
                    st.session_state.search_query = query
                    st.success(f"âœ… Found {len(results)} real content URLs")
                else:
                    st.warning("No results found")
        else:
            st.warning("Please enter a search query")
    
    # Results Display
    if hasattr(st.session_state, 'search_results'):
        st.markdown("---")
        st.subheader("ğŸ¯ Real Content URLs")
        
        results = st.session_state.search_results
        
        for i, result in enumerate(results, 1):
            # ä¿¡é ¼åº¦ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
            confidence = result.get('confidence', 0.5)
            if confidence >= 0.8:
                border_color = "#28a745"  # Green
            elif confidence >= 0.6:
                border_color = "#ffc107"  # Yellow
            else:
                border_color = "#dc3545"  # Red
            
            with st.container():
                st.markdown(f"""
                <div style="border: 2px solid {border_color}; border-radius: 10px; padding: 15px; margin: 10px 0;">
                    <div style="display: flex; justify-content: space-between; align-items: center;">
                        <div style="flex: 1;">
                            <h4 style="margin: 0; color: #333;">{result['title']}</h4>
                            <p style="margin: 5px 0; color: #666; font-size: 14px;">
                                <strong>URL:</strong> {result['url']}
                            </p>
                            <p style="margin: 5px 0; color: #666; font-size: 14px;">
                                <strong>Description:</strong> {result['description']}
                            </p>
                            <p style="margin: 5px 0; color: #888; font-size: 12px;">
                                <strong>Source:</strong> {result.get('source', 'Unknown')} | 
                                <strong>Confidence:</strong> {confidence:.1f}/1.0
                            </p>
                        </div>
                        <div style="margin-left: 20px;">
                            <a href="{result['url']}" target="_blank" style="text-decoration: none; margin-right: 10px;">ğŸ”—</a>
                        </div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # è©³ç´°åˆ†æãƒœã‚¿ãƒ³
                if st.button(f"ğŸ“„ Analyze Content", key=f"analyze_{i}"):
                    with st.spinner("Analyzing content..."):
                        analysis = fetch_and_analyze_content(
                            result['url'], 
                            st.session_state.search_query, 
                            api_key
                        )
                        
                        if analysis['success']:
                            st.markdown("---")
                            st.subheader(f"ğŸ” Content Analysis: {analysis['title']}")
                            
                            st.markdown(f"""
                            <div style="background-color: #f8f9fa; padding: 20px; border-radius: 10px;">
                            {analysis['analysis']}
                            </div>
                            """, unsafe_allow_html=True)
                            
                            st.write(f"**Content Length:** {analysis['content_length']} characters")
                        else:
                            st.error(f"âŒ Analysis failed: {analysis['error']}")
    
    # Clear button
    if st.button("ğŸ—‘ï¸ Clear Results"):
        for key in ['search_results', 'search_query']:
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()

if __name__ == "__main__":
    main()