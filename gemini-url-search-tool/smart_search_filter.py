#!/usr/bin/env python3
"""
Smart Search & Filter Tool - æ¤œç´¢â†’URLçµã‚Šè¾¼ã¿â†’è¦ç´„ã®å®Œå…¨ç‰ˆ
"""

import streamlit as st
import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import google.generativeai as genai
import time
from urllib.parse import quote_plus, urlparse
import re

# Load environment variables
load_dotenv()

def search_with_google_fallback(query, num_results=10):
    """Googleæ¤œç´¢ã®ä»£æ›¿ã¨ã—ã¦ã€è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™"""
    results = []
    
    # Method 1: Wikipediaæ¤œç´¢
    try:
        wiki_url = f"https://en.wikipedia.org/w/api.php?action=opensearch&search={quote_plus(query)}&limit=3&format=json"
        response = requests.get(wiki_url, timeout=10)
        data = response.json()
        
        if len(data) >= 4:
            titles = data[1]
            descriptions = data[2]
            urls = data[3]
            
            for i in range(min(len(titles), 3)):
                results.append({
                    'title': f"{titles[i]} - Wikipedia",
                    'url': urls[i],
                    'description': descriptions[i] if i < len(descriptions) else "Wikipedia article",
                    'source': 'Wikipedia'
                })
    except:
        pass
    
    # Method 2: GitHubæ¤œç´¢
    try:
        github_url = f"https://api.github.com/search/repositories?q={quote_plus(query)}&sort=stars&order=desc&per_page=3"
        response = requests.get(github_url, timeout=10)
        data = response.json()
        
        if 'items' in data:
            for repo in data['items'][:3]:
                results.append({
                    'title': f"{repo['name']} - GitHub",
                    'url': repo['html_url'],
                    'description': repo.get('description', 'GitHub repository'),
                    'source': 'GitHub'
                })
    except:
        pass
    
    # Method 3: æ‰‹å‹•ã§é–¢é€£ã‚µã‚¤ãƒˆã‚’è¿½åŠ 
    manual_sites = {
        'python': [
            {'title': 'Python.org - Official Site', 'url': 'https://www.python.org/', 'description': 'Official Python website'},
            {'title': 'Python Documentation', 'url': 'https://docs.python.org/3/', 'description': 'Official Python documentation'},
            {'title': 'Real Python', 'url': 'https://realpython.com/', 'description': 'Python tutorials and articles'},
        ],
        'machine learning': [
            {'title': 'Scikit-learn', 'url': 'https://scikit-learn.org/', 'description': 'Machine learning library for Python'},
            {'title': 'TensorFlow', 'url': 'https://www.tensorflow.org/', 'description': 'Open source machine learning platform'},
        ],
        'javascript': [
            {'title': 'MDN Web Docs', 'url': 'https://developer.mozilla.org/en-US/docs/Web/JavaScript', 'description': 'JavaScript documentation'},
            {'title': 'JavaScript.info', 'url': 'https://javascript.info/', 'description': 'Modern JavaScript tutorial'},
        ]
    }
    
    query_lower = query.lower()
    for keyword, sites in manual_sites.items():
        if keyword in query_lower:
            for site in sites:
                site['source'] = 'Manual'
                results.append(site)
    
    return results[:num_results]

def filter_urls_with_ai(results, query, api_key):
    """AIã‚’ä½¿ã£ã¦URLã‚’é–¢é€£æ€§ã§çµã‚Šè¾¼ã¿"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        results_text = "\n".join([
            f"{i+1}. {r['title']} - {r['url']} - {r['description']}"
            for i, r in enumerate(results)
        ])
        
        filter_prompt = f"""
æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’é–¢é€£æ€§ã®é«˜ã„é †ã«ä¸¦ã³æ›¿ãˆã¦ã€ä¸Šä½5ã¤ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

ã€æ¤œç´¢çµæœã€‘
{results_text}

ã€é¸æŠåŸºæº–ã€‘
1. æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£æ€§
2. æƒ…å ±ã®ä¿¡é ¼æ€§ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆã€æœ‰åã‚µã‚¤ãƒˆå„ªå…ˆï¼‰
3. å†…å®¹ã®å……å®Ÿåº¦

ã€å›ç­”å½¢å¼ã€‘
é¸æŠã—ãŸçµæœã®ç•ªå·ã‚’é–¢é€£æ€§ã®é«˜ã„é †ã«5ã¤ã¾ã§ã€ç†ç”±ã¨å…±ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ä¾‹: 1, 3, 5, 2, 7
ç†ç”±: 1ç•ªã¯å…¬å¼ã‚µã‚¤ãƒˆã§æœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„ã€3ç•ªã¯...

ç•ªå·ã®ã¿:
"""
        
        response = model.generate_content(filter_prompt)
        
        # ç•ªå·ã‚’æŠ½å‡º
        numbers = re.findall(r'\b(\d+)\b', response.text)
        selected_indices = []
        
        for num_str in numbers[:5]:  # ä¸Šä½5ã¤ã¾ã§
            try:
                idx = int(num_str) - 1  # 1-based to 0-based
                if 0 <= idx < len(results):
                    selected_indices.append(idx)
            except:
                continue
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¤ã¤é †åºã‚’ä¿æŒ
        unique_indices = []
        for idx in selected_indices:
            if idx not in unique_indices:
                unique_indices.append(idx)
        
        filtered_results = [results[i] for i in unique_indices]
        
        return filtered_results, response.text
        
    except Exception as e:
        st.warning(f"AI filtering failed: {e}. Using original results.")
        return results[:5], "AI filtering unavailable"

def fetch_page_content(url, timeout=15):
    """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title = soup.find('title')
        page_title = title.get_text().strip() if title else "No Title"
        
        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        if len(text) > 8000:
            text = text[:8000] + "..."
        
        return {
            'title': page_title,
            'content': text,
            'url': response.url,
            'success': True
        }
        
    except Exception as e:
        return {
            'error': str(e),
            'success': False
        }

def summarize_content(content_data, query, api_key):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¦ç´„"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®è¦³ç‚¹ã‹ã‚‰è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€ãƒšãƒ¼ã‚¸æƒ…å ±ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {content_data['title']}
URL: {content_data['url']}

ã€è¦ç´„æŒ‡é‡ã€‘
1. æ¤œç´¢ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹æƒ…å ±ã‚’é‡ç‚¹çš„ã«æŠ½å‡º
2. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3-4å€‹ã«æ•´ç†
3. å®Ÿç”¨çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§200-300æ–‡å­—

ã€å†…å®¹ã€‘
{content_data['content'][:6000]}

ã€è¦ç´„ã€‘
"""
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"

def main():
    st.set_page_config(
        page_title="Smart Search & Filter Tool",
        page_icon="ğŸ¯",
        layout="wide"
    )
    
    st.title("ğŸ¯ Smart Search & Filter Tool")
    st.markdown("**æ¤œç´¢ â†’ AIçµã‚Šè¾¼ã¿ â†’ å†…å®¹è¦ç´„**")
    st.markdown("---")
    
    # API key check
    api_key = os.getenv("GEMINI_API_KEY")
    
    if api_key:
        st.success("âœ… Gemini API key configured")
    else:
        st.error("âŒ Gemini API key not found")
        st.stop()
    
    # Step 1: Search
    st.subheader("ğŸ” Step 1: Search")
    
    query = st.text_input(
        "Search Query",
        placeholder="ä¾‹: Python æ©Ÿæ¢°å­¦ç¿’",
        help="æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›"
    )
    
    num_results = st.slider("Initial Results", 5, 15, 10)
    
    if st.button("ğŸ” Search", type="primary"):
        if query:
            with st.spinner("Searching..."):
                results = search_with_google_fallback(query, num_results)
                
                if results:
                    st.session_state.search_results = results
                    st.session_state.search_query = query
                    st.success(f"âœ… Found {len(results)} results")
                else:
                    st.warning("No results found")
        else:
            st.warning("Please enter a search query")
    
    # Step 2: AI Filtering
    if hasattr(st.session_state, 'search_results'):
        st.markdown("---")
        st.subheader("ğŸ¯ Step 2: AI Filtering")
        
        st.write(f"**Original Results:** {len(st.session_state.search_results)}")
        
        if st.button("ğŸ¤– Filter with AI"):
            with st.spinner("AI is filtering results..."):
                filtered_results, reasoning = filter_urls_with_ai(
                    st.session_state.search_results,
                    st.session_state.search_query,
                    api_key
                )
                
                st.session_state.filtered_results = filtered_results
                st.session_state.filter_reasoning = reasoning
                
                st.success(f"âœ… Filtered to {len(filtered_results)} most relevant results")
                
                with st.expander("ğŸ§  AI Reasoning"):
                    st.write(reasoning)
    
    # Step 3: Content Analysis
    if hasattr(st.session_state, 'filtered_results'):
        st.markdown("---")
        st.subheader("ğŸ“„ Step 3: Content Analysis")
        
        for i, result in enumerate(st.session_state.filtered_results, 1):
            with st.expander(f"Result {i}: {result['title']}", expanded=True):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write(f"**URL:** {result['url']}")
                    st.write(f"**Description:** {result['description']}")
                    st.write(f"**Source:** {result.get('source', 'Unknown')}")
                
                with col2:
                    if st.button(f"ğŸ“„ Analyze", key=f"analyze_{i}"):
                        with st.spinner(f"Analyzing content..."):
                            content_data = fetch_page_content(result['url'])
                            
                            if content_data['success']:
                                st.success("âœ… Content fetched")
                                
                                summary = summarize_content(
                                    content_data,
                                    st.session_state.search_query,
                                    api_key
                                )
                                
                                st.subheader("ğŸ¤– AI Summary")
                                st.markdown(f"""
                                <div style="background-color: #f0f2f6; padding: 15px; border-radius: 10px;">
                                {summary}
                                </div>
                                """, unsafe_allow_html=True)
                                
                                # Save summary
                                if 'summaries' not in st.session_state:
                                    st.session_state.summaries = []
                                
                                st.session_state.summaries.append({
                                    'title': result['title'],
                                    'url': result['url'],
                                    'summary': summary
                                })
                            else:
                                st.error(f"âŒ Failed: {content_data['error']}")
    
    # Combined Analysis
    if hasattr(st.session_state, 'summaries') and len(st.session_state.summaries) >= 2:
        st.markdown("---")
        st.subheader("ğŸ¯ Combined Analysis")
        
        if st.button("ğŸ”„ Generate Combined Summary"):
            with st.spinner("Generating combined analysis..."):
                all_summaries = "\n\n".join([
                    f"ã€{s['title']}ã€‘\n{s['summary']}"
                    for s in st.session_state.summaries
                ])
                
                try:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel('gemini-1.5-flash')
                    
                    combined_prompt = f"""
æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{st.session_state.search_query}ã€ã«ã¤ã„ã¦ã€è¤‡æ•°ã®Webã‚µã‚¤ãƒˆã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’çµ±åˆåˆ†æã—ã¦ãã ã•ã„ã€‚

ã€å„ã‚µã‚¤ãƒˆã®è¦ç´„ã€‘
{all_summaries}

ã€çµ±åˆåˆ†æã®è¦æ±‚ã€‘
1. å…±é€šã™ã‚‹é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®š
2. å„ã‚µã‚¤ãƒˆç‹¬è‡ªã®æœ‰ç”¨ãªæƒ…å ±ã‚’æ•´ç†
3. æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‚’æ¨å¥¨
4. å…¨ä½“çš„ãªçµè«–ã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ

ã€çµ±åˆåˆ†æçµæœã€‘
"""
                    
                    response = model.generate_content(combined_prompt)
                    
                    st.markdown(f"""
                    <div style="background-color: #e8f5e8; padding: 20px; border-radius: 10px; border-left: 5px solid #28a745;">
                    <h4>ğŸ¯ çµ±åˆåˆ†æçµæœ</h4>
                    {response.text}
                    </div>
                    """, unsafe_allow_html=True)
                    
                except Exception as e:
                    st.error(f"Combined analysis failed: {e}")
    
    # Clear all
    if st.button("ğŸ—‘ï¸ Clear All"):
        for key in ['search_results', 'filtered_results', 'summaries', 'search_query', 'filter_reasoning']:
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()

if __name__ == "__main__":
    main()